{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse des Films et Recommandations avec PySpark\n",
    "\n",
    "Dans ce notebook, nous allons explorer un ensemble de données de films et de notes de spectateurs, puis créer un système de recommandations en utilisant la bibliothèque PySpark. Nous utiliserons diverses fonctionnalités de Spark, comme les transformations de données, les calculs statistiques, ainsi que la réduction de dimension et la standardisation des données.\n",
    "\n",
    "## Étape 1 : Importation des bibliothèques et configuration de l'environnement\n",
    "Nous commençons par importer les bibliothèques nécessaires et configurer la session Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "try:\n",
    "    import distutils\n",
    "except ModuleNotFoundError:\n",
    "    import setuptools._distutils as distutils\n",
    "    sys.modules[\"distutils\"] = distutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette partie du script gère l'importation des bibliothèques nécessaires et traite un problème potentiel de module manquant (distutils). Si le module distutils n'est pas disponible, il essaie d'importer setuptools._distutils et le remplace dans le module système."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 2 : Initialisation de la session Spark\n",
    "Nous initialisons une session Spark avec 4 Go de mémoire allouée pour le driver et les exécutants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, count\n",
    "from pyspark.sql.functions import explode, split\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MoviesRatings\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 3 : Chargement des données\n",
    "Nous chargeons les fichiers CSV contenant les informations des films et des notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = spark.read.csv(\"ml-latest-small/movies.csv\", header=True, inferSchema=True)\n",
    "ratings = spark.read.csv(\"ml-latest-small/ratings.csv\", header=True, inferSchema=True)\n",
    "movies.show(10)\n",
    "ratings.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous chargeons les fichiers CSV contenant les informations des films et des notes. header=True indique que la première ligne des fichiers CSV contient les en-têtes de colonne, et inferSchema=True permet à Spark de deviner automatiquement les types de données des colonnes. Les fonctions show(10) affichent les 10 premières lignes des DataFrames movies et ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 4 : Exploration des données\n",
    "Affichage des schémas des deux ensembles de données pour mieux comprendre leur structure. Puis nous examinons le nombre total de film, les notes des films et le nombre d'utilisateurs uniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.printSchema()\n",
    "ratings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Nombre total de films : {movies.count()}\")\n",
    "print(f\"Nombre total de notes : {ratings.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'utilisateurs uniques : 610\n",
      "Nombre de films notés : 9724\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nombre d'utilisateurs uniques : {ratings.select('userId').distinct().count()}\")\n",
    "print(f\"Nombre de films notés : {ratings.select('movieId').distinct().count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette étape affiche les schémas des deux ensembles de données pour mieux comprendre leur structure. Ensuite, nous comptons le nombre total de films, le nombre total de notes, le nombre d'utilisateurs uniques et le nombre de films notés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 5 : Statistiques descriptives\n",
    "Affichage de la répartition des notes données par les utilisateurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.describe(\"rating\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "ratings.groupBy(\"rating\").count().orderBy(col(\"rating\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_movies = ratings.groupBy(\"movieId\") \\\n",
    "    .agg(avg(\"rating\").alias(\"avg_rating\"), count(\"rating\").alias(\"count_rating\")) \\\n",
    "    .filter(\"count_rating >= 10\") \\\n",
    "    .orderBy(col(\"avg_rating\").desc())\n",
    "best_movies.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_ratings_per_movie = ratings.groupBy(\"movieId\").count().agg({\"count\": \"avg\"})\n",
    "avg_ratings_per_movie.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_genres = movies.withColumn(\"genre\", explode(split(col(\"genres\"), \"\\\\|\")))\n",
    "movies_genres.groupBy(\"genre\").count().orderBy(col(\"count\").desc()).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette étape, nous affichons la répartition des notes données par les utilisateurs. Nous utilisons groupBy pour regrouper les notes par valeurs uniques et les compter. Ensuite, nous affichons les films les mieux notés en calculant la note moyenne (avg_rating) et le nombre de notes (count_rating) pour chaque film. Nous filtrons pour inclure uniquement les films avec au moins 10 notes. Nous calculons également la moyenne des évaluations par film et explorons les genres de films en décomposant les genres multiples en lignes distinctes (explode et split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies=movies.na.drop()\n",
    "movies.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings=ratings.na.drop()\n",
    "ratings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_r=movies.join(ratings,on=\"movieId\",how=\"inner\")\n",
    "movies.show()\n",
    "movies_r.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous supprimons les valeurs nulles dans les deux ensembles de données (na.drop()). Ensuite, nous effectuons une jointure interne (inner join) des données de films et des évaluations sur la colonne movieId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convertion du dataframe Spark des ratings et création de l'histogramme de répartion des notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ratings_pd = ratings.select(\"rating\").toPandas()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(ratings_pd['rating'], bins=10, kde=True, color='blue')\n",
    "plt.title(\"Répartition des notes\", fontsize=15)\n",
    "plt.xlabel(\"Notes\", fontsize=12)\n",
    "plt.ylabel(\"Nombre d'occurences\", fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation des genres les plus populaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_genres_pd = movies_genres.groupBy(\"genre\").count().orderBy(col(\"count\").desc()).toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=movies_genres_pd, x=\"count\", y=\"genre\", palette=\"viridis\")\n",
    "plt.title(\"Nombre de films par genre\", fontsize=15)\n",
    "plt.xlabel(\"Nombre de films\", fontsize=12)\n",
    "plt.ylabel(\"Genre\", fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation des 10 films les plus notés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_movies_pd = best_movies.join(movies, on=\"movieId\").select(\"title\", \"avg_rating\").orderBy(col(\"avg_rating\").desc()).limit(10).toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=best_movies_pd, x=\"avg_rating\", y=\"title\", palette=\"coolwarm\")\n",
    "plt.title(\"Top 10 des films les mieux notés\", fontsize=15)\n",
    "plt.xlabel(\"Note moyenne\", fontsize=12)\n",
    "plt.ylabel(\"Titre du film\", fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 6 : Modèle de recommandation avec ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous utilisons l'algorithme ALS (Alternating Least Squares) pour créer un modèle de recommandation. Nous divisons les données en un ensemble d'entraînement et un ensemble de test, entraînons le modèle, et faisons des prédictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test= movies_r.randomSplit([0.8,0.2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test= movies_r.randomSplit([0.8,0.2])\n",
    "als=ALS(maxIter=10, regParam=0.01,userCol=\"userId\",itemCol=\"movieId\",ratingCol=\"rating\",coldStartStrategy=\"drop\",rank=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Division des données : Nous divisons les données en ensembles d'entraînement (x_train) et de test (x_test) avec une répartition de 80% pour l'entraînement et 20% pour le test.\n",
    "\n",
    "Entraînement du modèle : Nous définissons et entraînons un modèle ALS avec 10 itérations (maxIter=10), un paramètre de régularisation de 0.01 (regParam=0.01), et une stratégie pour gérer les valeurs manquantes (coldStartStrategy=\"drop\"). Le modèle est ensuite ajusté (fit) sur les données d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=als.fit(x_train)\n",
    "pred=model.transform(x_test)\n",
    "pred.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prédictions : Nous faisons des prédictions sur les données de test et les affichons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval=RegressionEvaluator(metricName=\"rmse\",labelCol=\"rating\",predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Évaluation du modèle : Nous évaluons la performance du modèle en utilisant l'erreur quadratique moyenne (RMSE) avec RegressionEvaluator.\n",
    "\n",
    "Recommandations : Nous générons des recommandations pour tous les utilisateurs et tous les films, et nous les affichons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
